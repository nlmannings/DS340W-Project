{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEzxgz-2oBAe"
      },
      "outputs": [],
      "source": [
        "# kernel_search_metrics.py\n",
        "\"\"\"\n",
        "Grid search over kernel sizes (conv1, conv2) with Stratified K-Fold evaluation.\n",
        "Saves aggregated metrics (mean + std over folds) to kernel_search_results.csv.\n",
        "\n",
        "Notes:\n",
        "- Uses user's loader (expects PNG scalograms in the two folders).\n",
        "- Metrics: balanced accuracy, precision, recall, F1 (binary averaging, schizophrenia = positive class (1)).\n",
        "- Uses TF/Keras for model; scikit-learn for metrics and cross-validation.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import layers, models, regularizers, callbacks, optimizers\n",
        "import tensorflow as tf\n",
        "\n",
        "# Path to uploaded notebook (provided in history) â€” kept here per request.\n",
        "UPLOADED_NOTEBOOK_PATH = \"/mnt/data/DS 340W Project.ipynb\"\n",
        "\n",
        "# ---------------------------\n",
        "# 0. User-editable config\n",
        "# ---------------------------\n",
        "KERNEL_SIZES = [(3,3), (5,5), (7,7)]\n",
        "N_SPLITS = 5  # Stratified K-Fold splits\n",
        "RANDOM_SEED = 42\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "DROPOUT_P = 0.1\n",
        "RESULTS_CSV = \"kernel_search_results.csv\"\n",
        "TARGET_SIZE = (224, 224)  # as in your loader\n",
        "POS_LABEL = 1  # schizophrenia class is positive\n",
        "VERBOSE = 1\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Try to mount Google Drive (Colab)\n",
        "# ---------------------------\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Mounted Google Drive at /content/drive\")\n",
        "except Exception:\n",
        "    print(\"google.colab.drive.mount not available. If not running in Colab, ensure the dataset paths below exist.\")\n",
        "\n",
        "# Update these paths if needed (keeps your original)\n",
        "healthy_save_path = '/content/drive/MyDrive/DS340W/Data/dataset2/healthy'\n",
        "schizophrenia_save_path = '/content/drive/MyDrive/DS340W/Data/dataset2/schizophrenic'\n",
        "\n",
        "healthy_folder = healthy_save_path\n",
        "schizophrenia_folder = schizophrenia_save_path\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Load & preprocess images (your loader adapted)\n",
        "# ---------------------------\n",
        "def load_images(folder, label_for_folder):\n",
        "    images, labels = [], []\n",
        "    if not os.path.exists(folder):\n",
        "        raise FileNotFoundError(f\"Folder not found: {folder}\")\n",
        "    for filename in sorted(os.listdir(folder)):\n",
        "        if filename.lower().endswith(\".png\"):\n",
        "            img = image.load_img(os.path.join(folder, filename), target_size=TARGET_SIZE)\n",
        "            img_array = image.img_to_array(img) / 255.0\n",
        "            images.append(img_array)\n",
        "            labels.append(label_for_folder)\n",
        "    if len(images) == 0:\n",
        "        raise ValueError(f\"No PNG images found in folder: {folder}\")\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "print(\"Loading healthy images...\")\n",
        "X_healthy, y_healthy = load_images(healthy_folder, 0)\n",
        "print(\"Loading schizophrenia images...\")\n",
        "X_schizophrenia, y_schizophrenia = load_images(schizophrenia_folder, 1)\n",
        "\n",
        "# Merge and shuffle\n",
        "X = np.concatenate((X_healthy, X_schizophrenia), axis=0)\n",
        "y = np.concatenate((y_healthy, y_schizophrenia), axis=0)\n",
        "rng = np.random.RandomState(RANDOM_SEED)\n",
        "perm = rng.permutation(len(y))\n",
        "X = X[perm]\n",
        "y = y[perm]\n",
        "\n",
        "print(f\"Loaded dataset: X.shape={X.shape}, y.shape={y.shape}, #HC={np.sum(y==0)}, #SCZ={np.sum(y==1)}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Model builder\n",
        "# ---------------------------\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def build_model(kernel_size_1=(3,3), kernel_size_2=(3,3), input_shape=None, dropout_p=0.1):\n",
        "    if input_shape is None:\n",
        "        input_shape = X.shape[1:]\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(filters=4, kernel_size=kernel_size_1, strides=(2,2),\n",
        "                      padding='same', activation='relu')(inp)\n",
        "    x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
        "    x = layers.Dropout(dropout_p)(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=8, kernel_size=kernel_size_2, strides=(2,2),\n",
        "                      padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.GlobalMaxPool2D()(x)\n",
        "    x = layers.Dropout(dropout_p)(x)\n",
        "\n",
        "    x = layers.Dense(50, activation='relu')(x)\n",
        "    out = layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Try to allow GPU memory growth if GPUs available\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for g in gpus:\n",
        "            tf.config.experimental.set_memory_growth(g, True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Grid search with Stratified K-Fold\n",
        "# ---------------------------\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "total_runs = len(KERNEL_SIZES) * len(KERNEL_SIZES)\n",
        "run_counter = 0\n",
        "\n",
        "for ks1, ks2 in [(a,b) for a in KERNEL_SIZES for b in KERNEL_SIZES]:\n",
        "    run_counter += 1\n",
        "    print(f\"\\n=== Kernel run {run_counter}/{total_runs}: conv1={ks1}, conv2={ks2} ===\")\n",
        "\n",
        "    # Collect fold metrics\n",
        "    fold_bal_acc = []\n",
        "    fold_prec = []\n",
        "    fold_rec = []\n",
        "    fold_f1 = []\n",
        "    fold_epochs = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X, y):\n",
        "        fold_idx += 1\n",
        "        print(f\"  Fold {fold_idx}/{N_SPLITS}\")\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Keras expects categorical labels\n",
        "        y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
        "        y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes=2)\n",
        "\n",
        "        # Build model for this fold\n",
        "        model = build_model(kernel_size_1=ks1, kernel_size_2=ks2, input_shape=X_train.shape[1:], dropout_p=DROPOUT_P)\n",
        "\n",
        "        # Callbacks\n",
        "        es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train_cat,\n",
        "            validation_data=(X_val, y_val_cat),\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            callbacks=[es],\n",
        "            verbose=VERBOSE\n",
        "        )\n",
        "\n",
        "        fold_epochs.append(len(history.history['loss']))\n",
        "\n",
        "        # Predict\n",
        "        y_pred_prob = model.predict(X_val, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "        # Compute metrics (binary averaging; positive label = POS_LABEL)\n",
        "        bal_acc = balanced_accuracy_score(y_val, y_pred)\n",
        "        prec = precision_score(y_val, y_pred, pos_label=POS_LABEL, zero_division=0)\n",
        "        rec = recall_score(y_val, y_pred, pos_label=POS_LABEL, zero_division=0)\n",
        "        f1 = f1_score(y_val, y_pred, pos_label=POS_LABEL, zero_division=0)\n",
        "\n",
        "        print(f\"    fold bal_acc={bal_acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n",
        "\n",
        "        fold_bal_acc.append(bal_acc)\n",
        "        fold_prec.append(prec)\n",
        "        fold_rec.append(rec)\n",
        "        fold_f1.append(f1)\n",
        "\n",
        "        # Clean up to reduce GPU memory growth between folds\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "    # Aggregate per kernel combo (mean + std)\n",
        "    result = {\n",
        "        \"kernel_size_conv1\": f\"{ks1[0]}x{ks1[1]}\",\n",
        "        \"kernel_size_conv2\": f\"{ks2[0]}x{ks2[1]}\",\n",
        "        \"bal_acc_mean\": float(np.mean(fold_bal_acc)),\n",
        "        \"bal_acc_std\": float(np.std(fold_bal_acc)),\n",
        "        \"precision_mean\": float(np.mean(fold_prec)),\n",
        "        \"precision_std\": float(np.std(fold_prec)),\n",
        "        \"recall_mean\": float(np.mean(fold_rec)),\n",
        "        \"recall_std\": float(np.std(fold_rec)),\n",
        "        \"f1_mean\": float(np.mean(fold_f1)),\n",
        "        \"f1_std\": float(np.std(fold_f1)),\n",
        "        \"mean_train_epochs\": float(np.mean(fold_epochs))\n",
        "    }\n",
        "    all_results.append(result)\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Save results and print summary\n",
        "# ---------------------------\n",
        "df = pd.DataFrame(all_results)\n",
        "df = df.sort_values(by=\"f1_mean\", ascending=False).reset_index(drop=True)\n",
        "df.to_csv(RESULTS_CSV, index=False)\n",
        "print(f\"\\nGrid search complete. Aggregated results saved to: {RESULTS_CSV}\")\n",
        "print(df)\n",
        "\n",
        "# Also show the uploaded notebook path (per your developer request)\n",
        "print(\"\\nUploaded notebook path (from conversation history):\")\n",
        "print(UPLOADED_NOTEBOOK_PATH)\n"
      ]
    }
  ]
}